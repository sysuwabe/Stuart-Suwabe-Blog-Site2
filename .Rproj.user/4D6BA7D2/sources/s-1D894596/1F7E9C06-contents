---
title: "Mid-term Practice"
author: "Jameson Watts, Ph.D."
date: "03/08/2021"
output: 
  html_document:
    df_print: kable
    fig_width: 11
    fig_height: 8
---

# Setup 

```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(caret)
library(fastDummies)
library(rpart)
library(rpart.plot)

wine = read_rds("/Users/stuartsuwabe/Downloads/pinot.rds") %>% 
  mutate(lprice=log(price)) %>% 
  mutate(fruit=as.integer(str_detect(description,"[Ff]ruit"))) %>% 
  select(-description, -price, -taster_name) 

```

# Multiple Regression and Feature Engineering

Use the 'train' function from the Caret library to run a linear regression model on the full dataset with the natural log of price as the dependent variable and all other columns as predictors. 

*Hint:* Use 'trControl = trainControl(number = 1)' to make things go faster.

```{r}
set.seed(504)
m1 <- train(lprice ~ .,
            data = wine, 
            method = "lm", 
            trControl = trainControl(number = 1))

summary(m1)

library(modelr)
rmse(m1, wine) 
```

## Preprocessing

Run the same model as in the last question, but this time preprocess the predictors using Box-Cox transformations, centering and scaling. 

*Hint:* As a shortcut, you can perform this directly in the 'train' function by passing a list of transformation functions to the 'preProcess' parameter.

```{r}
set.seed(504)
m2 <- train(lprice ~ .,
            data = wine, 
            method = "lm", 
            trControl = trainControl(number = 1), 
            preProcess = c("BoxCox", "center", "scale"))

summary(m2)

rmse(m2, wine) 
```

## Diagnostics

Is there a meaningful difference in the performance of the linear regression with and without transforming the predictors? Use the RMSE of each model to justify your answer, then explain the difference (or lack of difference) based on how Box-Cox transformations work.

**Answer:** There does not seem to be a meaningful difference in the linear regression between the two models. Without transformation, the RMSE is 0.3979 while with the transformation it is 0.3974, hardly a difference. Box-Cox transformations transforms our data into a normal distribution, but distribution is already pretty normal in our data set, as are out continuous predictor variables. 

## Interpretation

In the models you just ran, what does the coefficient estimate on the 'year' variable mean?

**Answer:** The coefficient estimate of the year variable is how said variable relates to the intercept, in this case the log-transformed price. For instance, in the transformed model, keeping all levels constant, as year increases, the price decreases by -3%.  

# Interactions

Use the 'train' function from the Caret library to run a linear regression model with the equation: lprice ~ fruit*points

*Hint:* Run the summary() function on your saved model to see the coefficient estimates. For instance, if I saved my model as 'fit' I would call 'summary(fit)'.

```{r}
set.seed(504)
m3 <- train(lprice ~ fruit*points,
            data = wine, 
            method = "lm", 
            trControl = trainControl(number = 1))

summary(m3)
```

## Interpretation

How should I interpret the coefficient on the interaction variable? Please explain as you would to a non-technical manager.

**Answer:** If fruit is not present, as points increases by 1, there is a 10% increase in price. If fruit is present, as points increases by 1, there is a 12% increase in price; there is a steeper slope. 
If fruit exists, and there are points, the price increases. price increases more by 2% when fruit does not exist. 

If a wine is rated higher, than it has a higher price. As well, if 'fruit' is in the description of a wine, price rises faster with an increase in points. 

# Decision Trees

Use the train function from the Caret library to run a basic decision tree model to determine province. Use the entire dataset and a fixed complexity parameter of 0.01.

*Hint:* you can use "tuneGrid = expand.grid(cp = 0.01)" to set the complexity parameter within the train function

```{r}
ctrl <- trainControl(method = "cv")
fit <- train(province ~ ., data = wine,
             method = "rpart",
             trControl = ctrl,
             metric = "Kappa", 
             tuneGrid = expand.grid(cp = 0.01))

fit
```

## Visualize

Plot the resulting tree using the rpart.plot function

```{r}
rpart.plot(fit$finalModel, type = 5) 
```

## Interpretation

Use the plot of your decision tree to construct a coherent sentence that explains the data in the right-most, bottom leaf of the tree.

**Answer:** For this tree, basically, the bottommost right leaf indicates that if the wine is from before 2012 and has a logged price of less than 4.4 (or actual price of 81.45087), there is a 51% chance it will be from Oregon. 

# Logistic Regression 

Create a new variable that is True if a wine is from Oregon and false otherwise, then delete the province column.

```{r}
wine1 <- wine %>% 
  mutate(oregon = ifelse(province == "Oregon", 1, 0)) %>%
  select(-province)
```

## Sampling

Create training and test samples that are 70% and 30% of the full dataset respectively.

```{r}
set.seed(504)
wine_index <- createDataPartition(wine1$oregon, times = 1, p = 0.7, list = FALSE)
train <- wine1[ wine_index, ]
test <- wine1[-wine_index, ]
```

## Run the model

Use the train function from Caret to run a logistic regression model on the training data with province_Oregon as the dependent variable and all other variables as predictors.

```{r}
m1 <- train(oregon ~ .,
            data = train, 
            method = 'glm',
            family = "binomial",
            trControl = trainControl(number = 1))

summary(m1)
```

## Interpretation

Use the coefficient estimates from your logistic model to explain how the word 'fruit' in the description changes the probability that a Pinot is from Oregon. Please use language suitable for a non-technical manager.

```{r}
odds <- exp(coef(m1$finalModel))
data.frame(name = names(odds), odds = odds) %>%  
  mutate(probability=odds/(1+odds)) %>% 
  arrange(desc(odds)) %>% 
  head()
```

**Answer:** If fruit is in the description, there seems to be about a 73% chance it will be from Oregon. 

## Interpretation

Evaluate your logistic model against the test data using a confusion matrix. Have you achieved a good Kappa? Why or why not.

```{r}
prob <- predict(m1, newdata=test)
pred <- ifelse(prob > 0.5, 1, 0)

confusionMatrix(factor(pred),factor(test$oregon))
```

*Note:* Assume probabilities over 0.5 are equivalent to a positive prediction. 

**Answer:** This is not a good Kappa, which states we perform a paltry 15% better than simply choosing. 

## Bonus

In the logistic model you just ran, what does the coefficient estimate on the 'year' variable mean?

**Answer:** For every year increase, the chance of a Pinot being from Oregon decreases by about 13%. In other words, the Pinots coming from Oregon are decreasing over time. 


